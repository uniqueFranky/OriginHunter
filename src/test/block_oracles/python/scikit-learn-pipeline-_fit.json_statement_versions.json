{
  "statementVersions": [
    {
      "commit": "13981bdce97ab2dd49b6b8707f3f27b5c148b9c0",
      "startLine": 22,
      "endLine": 58,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            if '__' not in pname:\n                raise ValueError(\n                    \"Pipeline.fit does not accept the {} parameter. \"\n                    \"You can pass parameters to specific steps of your \"\n                    \"pipeline using the stepname__parameter format, e.g. \"\n                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                    \"=sample_weight)`.\".format(pname))\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        for (step_idx,\n             name,\n             transformer) in self._iter(with_final=False,\n                                        filter_passthrough=False):\n            if (transformer is None or transformer == 'passthrough'):\n                with _print_elapsed_time('Pipeline',\n                                         self._log_message(step_idx)):\n                    continue\n\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            X, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, X, y, None,\n                message_clsname='Pipeline',\n                message=self._log_message(step_idx),\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return X, {}\n        return X, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "8edd9f9f1597094a1f8024f4d091f21e9c1fa66a",
      "startLine": 23,
      "endLine": 59,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            if '__' not in pname:\n                raise ValueError(\n                    \"Pipeline.fit does not accept the {} parameter. \"\n                    \"You can pass parameters to specific steps of your \"\n                    \"pipeline using the stepname__parameter format, e.g. \"\n                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                    \"=sample_weight)`.\".format(pname))\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for (step_idx,\n             name,\n             transformer) in self._iter(with_final=False,\n                                        filter_passthrough=False):\n            if (transformer is None or transformer == 'passthrough'):\n                with _print_elapsed_time('Pipeline',\n                                         self._log_message(step_idx)):\n                    continue\n\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                message_clsname='Pipeline',\n                message=self._log_message(step_idx),\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "0e54f44b4e26b28819c1e745793fe5bec9b1cea9",
      "startLine": 23,
      "endLine": 49,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            if '__' not in pname:\n                raise ValueError(\n                    \"Pipeline.fit does not accept the {} parameter. \"\n                    \"You can pass parameters to specific steps of your \"\n                    \"pipeline using the stepname__parameter format, e.g. \"\n                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                    \"=sample_weight)`.\".format(pname))\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, name, transformer in self._iter(with_final=False):\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "0e3bb17e6263a391c8409558634b75f437eb3f27",
      "startLine": 16,
      "endLine": 42,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, name, transformer in self._iter(with_final=False):\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "952ef6637a13ba01055bf0ed9d29d525f0fee5bc",
      "startLine": 16,
      "endLine": 42,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in fit_params.items():\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, name, transformer in self._iter(with_final=False):\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "1b90237b45dfa1652831cfe72bd4b29a31b070ec",
      "startLine": 16,
      "endLine": 42,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, name, transformer in self._iter(with_final=False):\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "9f842e3496ab59ad593b46553fadff7b2490ef1a",
      "startLine": 16,
      "endLine": 43,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(\n                self._iter(with_final=False)):\n            if hasattr(memory, 'location'):\n                # joblib >= 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            elif hasattr(memory, 'cachedir'):\n                # joblib < 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n            else:\n                cloned_transformer = clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer = fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator == 'passthrough':\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "47d3b3c30fd408187ce0524043be43bf378397dd",
      "startLine": 16,
      "endLine": 45,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, 'location'):\n                    # joblib >= 0.12\n                    if memory.location is None:\n                        # we do not clone when caching is disabled to\n                        # preserve backward compatibility\n                        cloned_transformer = transformer\n                    else:\n                        cloned_transformer = clone(transformer)\n                elif hasattr(memory, 'cachedir'):\n                    # joblib < 0.11\n                    if memory.cachedir is None:\n                        # we do not clone when caching is disabled to\n                        # preserve backward compatibility\n                        cloned_transformer = transformer\n                    else:\n                        cloned_transformer = clone(transformer)\n                else:\n                    cloned_transformer = clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer = fit_transform_one_cached(\n                    cloned_transformer, Xt, y, None,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "0b6308c2708fe03071cdbf24997eb967403f5965",
      "startLine": 16,
      "endLine": 33,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, 'cachedir') and memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer = fit_transform_one_cached(\n                    cloned_transformer, Xt, y, None,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "a65a9e53af4873729ee98de76f0e8afa6f39d8e5",
      "startLine": 16,
      "endLine": 33,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps = list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, 'cachedir') and memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer = fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "e66aa6dc70bb43541bd2a084ea28d0547b89ed4f",
      "startLine": 14,
      "endLine": 31,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        self._validate_steps()\n        # Setup the memory\n        memory = check_memory(self.memory)\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, 'cachedir') and memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer = fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "4c33ea2ee0587f47da3b7e2162ef29fff0ee7f93",
      "startLine": 23,
      "endLine": 40,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        self._validate_steps()\n        # Setup the memory\n        memory = self.memory\n        if memory is None:\n            memory = Memory(cachedir=None, verbose=0)\n        elif isinstance(memory, six.string_types):\n            memory = Memory(cachedir=memory, verbose=0)\n        elif not isinstance(memory, Memory):\n            raise ValueError(\"'memory' should either be a string or\"\n                             \" a sklearn.externals.joblib.Memory\"\n                             \" instance, got 'memory={!r}' instead.\".format(\n                                 type(memory)))\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer = fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "b3a639ffc2d518b8862c61e4170403a400368571",
      "startLine": 22,
      "endLine": 39,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        self._validate_steps()\n        # Setup the memory\n        memory = self.memory\n        if memory is None:\n            memory = Memory(cachedir=None, verbose=0)\n        elif isinstance(memory, six.string_types):\n            memory = Memory(cachedir=memory, verbose=0)\n        elif not isinstance(memory, Memory):\n            raise ValueError(\"'memory' should either be a string or\"\n                             \" a joblib.Memory instance, got\"\n                             \" 'memory={!r}' instead.\".format(memory))\n\n        fit_transform_one_cached = memory.cache(_fit_transform_one)\n\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer = transformer\n                else:\n                    cloned_transformer = clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer = fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] = (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    },
    {
      "commit": "5b20d484add50aec64a1bda5c52ed2ceb7557f36",
      "startLine": 9,
      "endLine": 16,
      "methodCode": "    def _fit(self, X, y=None, **fit_params):\n        self._validate_steps()\n        fit_params_steps = dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param = pname.split('__', 1)\n            fit_params_steps[step][param] = pval\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if transform is None:\n                pass\n            elif hasattr(transform, \"fit_transform\"):\n                Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n            else:\n                Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n                              .transform(Xt)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n"
    }
  ]
}