{
  "origin": "codeshovel",
  "repositoryName": "lucene-solr",
  "repositoryPath": "/Users/franky/Documents/Homework/毕业设计/testcase/lucene-solr/.git",
  "startCommitName": "38bf976cd4b9e324c21664bd7ae3d554df803705",
  "sourceFileName": "MemoryIndex.java",
  "functionName": "keywordTokenStream",
  "functionId": "keywordTokenStream___keywords-Collection__T__(modifiers-final)",
  "sourceFilePath": "lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
  "functionAnnotation": "",
  "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
  "functionStartLine": 334,
  "functionEndLine": 361,
  "numCommitsSeen": 278,
  "timeTaken": 3692,
  "changeHistory": [
    "e8e4245d9b36123446546ff15967ac95429ea2b0",
    "ad0e49591148340418569d7c650761a6d41cf1b1",
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
    "3f722b66a55ace117de6458f5b7d52f5bbc20c62",
    "786eb6ce0d19c6459f204b5d4ab0dc72245888cb",
    "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
    "ec90bc2202ef501e257eaf235be5ca15239c03c2",
    "1743081b078a3206e676bdd4ebe9203f5bad6c90",
    "bb6b7117186b656b4777850fdc463e0eaa541130",
    "e28541354d496a43078c1bc281076f97ed7d008c"
  ],
  "changeHistoryShort": {
    "e8e4245d9b36123446546ff15967ac95429ea2b0": "Yfilerename",
    "ad0e49591148340418569d7c650761a6d41cf1b1": "Ybodychange",
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec": "Yfilerename",
    "3f722b66a55ace117de6458f5b7d52f5bbc20c62": "Ybodychange",
    "786eb6ce0d19c6459f204b5d4ab0dc72245888cb": "Ybodychange",
    "786457c0e3ae6afa610788a9ffdfe6bf01c1f976": "Ymultichange(Yparameterchange,Ybodychange)",
    "ec90bc2202ef501e257eaf235be5ca15239c03c2": "Ybodychange",
    "1743081b078a3206e676bdd4ebe9203f5bad6c90": "Ybodychange",
    "bb6b7117186b656b4777850fdc463e0eaa541130": "Ybodychange",
    "e28541354d496a43078c1bc281076f97ed7d008c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e8e4245d9b36123446546ff15967ac95429ea2b0": {
      "type": "Yfilerename",
      "commitMessage": "LUCENE-3965: consolidate all api modules and fix packaging for 4.0\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1327094 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012/4/17 下午9:36",
      "commitName": "e8e4245d9b36123446546ff15967ac95429ea2b0",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2012/4/17 下午8:03",
      "commitNameOld": "0daa4b0aac1748bbb2c56547626e9f49e7fb4ed6",
      "commitAuthorOld": "Sami Siren",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private final CharTermAttribute termAtt \u003d addAttribute(CharTermAttribute.class);\n\n        private final OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        @Override\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            clearAttributes();\n            termAtt.setEmpty().append(term);\n            offsetAtt.setOffset(start, start + termAtt.length());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 282,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "",
      "extendedDetails": {
        "oldPath": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
        "newPath": "lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java"
      }
    },
    "ad0e49591148340418569d7c650761a6d41cf1b1": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2372: switch over remaining uses of TermAttribute\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@950008 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2010/6/1 下午6:35",
      "commitName": "ad0e49591148340418569d7c650761a6d41cf1b1",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2016/1/23 上午8:18",
      "commitNameOld": "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
      "commitAuthorOld": "Dawid Weiss",
      "daysBetweenCommits": -2061.57,
      "commitsBetweenForRepo": 0,
      "commitsBetweenForFile": 0,
      "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private final CharTermAttribute termAtt \u003d addAttribute(CharTermAttribute.class);\n\n        private final OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        @Override\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            clearAttributes();\n            termAtt.setEmpty().append(term);\n            offsetAtt.setOffset(start, start + termAtt.length());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 273,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "@@ -1,29 +1,29 @@\n public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n         private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n-        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n+        private final CharTermAttribute termAtt \u003d addAttribute(CharTermAttribute.class);\n \n-        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n+        private final OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n \n         @Override\n         public boolean incrementToken() {\n             if (!iter.hasNext())\n                 return false;\n             T obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n             clearAttributes();\n-            termAtt.setTermBuffer(term);\n-            offsetAtt.setOffset(start, start + termAtt.termLength());\n+            termAtt.setEmpty().append(term);\n+            offsetAtt.setOffset(start, start + termAtt.length());\n             start +\u003d term.length() + 1;\n             return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec": {
      "type": "Yfilerename",
      "commitMessage": "SVN-GIT conversion, path copy emulation.\n",
      "commitDate": "2016/1/23 上午8:18",
      "commitName": "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
      "commitAuthor": "Dawid Weiss",
      "commitDateOld": "2010/3/17 下午10:57",
      "commitNameOld": "2e5c6cdadc820220f8cb86e1b6e215da941649f9",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 2137.39,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        @Override\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            clearAttributes();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 271,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "",
      "extendedDetails": {
        "oldPath": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
        "newPath": "lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java"
      }
    },
    "3f722b66a55ace117de6458f5b7d52f5bbc20c62": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2211: Fix various missing clearAttributes() and improve BaseTokenStreamTestCase to check for this trap\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@899627 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2010/1/15 下午9:42",
      "commitName": "3f722b66a55ace117de6458f5b7d52f5bbc20c62",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2009/12/5 下午5:43",
      "commitNameOld": "fa65d42e942dda48fcdb0a7c87c6b5f71f71bb83",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 41.17,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        @Override\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            clearAttributes();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 270,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "@@ -1,28 +1,29 @@\n public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n         private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n         private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n \n         private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n \n         @Override\n         public boolean incrementToken() {\n             if (!iter.hasNext())\n                 return false;\n             T obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n+            clearAttributes();\n             termAtt.setTermBuffer(term);\n             offsetAtt.setOffset(start, start + termAtt.termLength());\n             start +\u003d term.length() + 1;\n             return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "786eb6ce0d19c6459f204b5d4ab0dc72245888cb": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2012: add remaining @overrides (contrib,demo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@833867 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009/11/8 下午8:45",
      "commitName": "786eb6ce0d19c6459f204b5d4ab0dc72245888cb",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2009/11/5 上午6:37",
      "commitNameOld": "80e8bfbbc9e4932d557d016a4f566f40459cb50d",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 3.59,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        @Override\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 270,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "@@ -1,27 +1,28 @@\n public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n         private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n         private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n \n         private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n \n+        @Override\n         public boolean incrementToken() {\n             if (!iter.hasNext())\n                 return false;\n             T obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n             termAtt.setTermBuffer(term);\n             offsetAtt.setOffset(start, start + termAtt.termLength());\n             start +\u003d term.length() + 1;\n             return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "786457c0e3ae6afa610788a9ffdfe6bf01c1f976": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "LUCENE-1257: Generics in contrib/memory, contrib/wordnet (previously memory), contrib/misc, contrib/benchmark\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@830790 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009/10/29 上午7:21",
      "commitName": "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
      "commitAuthor": "Uwe Schindler",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "LUCENE-1257: Generics in contrib/memory, contrib/wordnet (previously memory), contrib/misc, contrib/benchmark\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@830790 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2009/10/29 上午7:21",
          "commitName": "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
          "commitAuthor": "Uwe Schindler",
          "commitDateOld": "2009/10/26 下午10:55",
          "commitNameOld": "74f872182ed9699d0e1d16ca98ecf72d7622cb67",
          "commitAuthorOld": "Michael McCandless",
          "daysBetweenCommits": 2.35,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
          "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
          "functionStartLine": 273,
          "functionName": "keywordTokenStream",
          "functionAnnotation": "",
          "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
          "diff": "@@ -1,27 +1,27 @@\n-public TokenStream keywordTokenStream(final Collection keywords) {\n+public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n-        private Iterator iter \u003d keywords.iterator();\n+        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n         private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n \n         private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n \n         public boolean incrementToken() {\n             if (!iter.hasNext())\n                 return false;\n-            Object obj \u003d iter.next();\n+            T obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n             termAtt.setTermBuffer(term);\n             offsetAtt.setOffset(start, start + termAtt.termLength());\n             start +\u003d term.length() + 1;\n             return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "[keywords-Collection(modifiers-final)]",
            "newValue": "[keywords-Collection\u003cT\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "LUCENE-1257: Generics in contrib/memory, contrib/wordnet (previously memory), contrib/misc, contrib/benchmark\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@830790 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2009/10/29 上午7:21",
          "commitName": "786457c0e3ae6afa610788a9ffdfe6bf01c1f976",
          "commitAuthor": "Uwe Schindler",
          "commitDateOld": "2009/10/26 下午10:55",
          "commitNameOld": "74f872182ed9699d0e1d16ca98ecf72d7622cb67",
          "commitAuthorOld": "Michael McCandless",
          "daysBetweenCommits": 2.35,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "actualSource": "public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            T obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
          "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
          "functionStartLine": 273,
          "functionName": "keywordTokenStream",
          "functionAnnotation": "",
          "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
          "diff": "@@ -1,27 +1,27 @@\n-public TokenStream keywordTokenStream(final Collection keywords) {\n+public \u003cT\u003e TokenStream keywordTokenStream(final Collection\u003cT\u003e keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n-        private Iterator iter \u003d keywords.iterator();\n+        private Iterator\u003cT\u003e iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n         private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n \n         private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n \n         public boolean incrementToken() {\n             if (!iter.hasNext())\n                 return false;\n-            Object obj \u003d iter.next();\n+            T obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n             termAtt.setTermBuffer(term);\n             offsetAtt.setOffset(start, start + termAtt.termLength());\n             start +\u003d term.length() + 1;\n             return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
          "extendedDetails": {}
        }
      ]
    },
    "ec90bc2202ef501e257eaf235be5ca15239c03c2": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-1855: Change AttributeSource API to use generics\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@820553 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009/10/1 下午3:53",
      "commitName": "ec90bc2202ef501e257eaf235be5ca15239c03c2",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2009/8/27 上午7:14",
      "commitNameOld": "f39dadfb26311d99b266affd8eb62456f4fbeb9f",
      "commitAuthorOld": "Mark Robert Miller",
      "daysBetweenCommits": 35.36,
      "commitsBetweenForRepo": 105,
      "commitsBetweenForFile": 1,
      "actualSource": "public TokenStream keywordTokenStream(final Collection keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            Object obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 271,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "@@ -1,27 +1,27 @@\n public TokenStream keywordTokenStream(final Collection keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n         private Iterator iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n-        private TermAttribute termAtt \u003d (TermAttribute) addAttribute(TermAttribute.class);\n+        private TermAttribute termAtt \u003d addAttribute(TermAttribute.class);\n \n-        private OffsetAttribute offsetAtt \u003d (OffsetAttribute) addAttribute(OffsetAttribute.class);\n+        private OffsetAttribute offsetAtt \u003d addAttribute(OffsetAttribute.class);\n \n         public boolean incrementToken() {\n             if (!iter.hasNext())\n                 return false;\n             Object obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n             termAtt.setTermBuffer(term);\n             offsetAtt.setOffset(start, start + termAtt.termLength());\n             start +\u003d term.length() + 1;\n             return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "1743081b078a3206e676bdd4ebe9203f5bad6c90": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-1460: Changed TokenStreams/TokenFilters in contrib to use the new TokenStream API.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@799953 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009/8/2 上午6:52",
      "commitName": "1743081b078a3206e676bdd4ebe9203f5bad6c90",
      "commitAuthor": "Michael Busch",
      "commitDateOld": "2009/1/25 下午10:38",
      "commitNameOld": "c6f6f016433ffe8450e4b2f202c9ac5b848305ec",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 188.34,
      "commitsBetweenForRepo": 413,
      "commitsBetweenForFile": 1,
      "actualSource": "public TokenStream keywordTokenStream(final Collection keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        private TermAttribute termAtt \u003d (TermAttribute) addAttribute(TermAttribute.class);\n\n        private OffsetAttribute offsetAtt \u003d (OffsetAttribute) addAttribute(OffsetAttribute.class);\n\n        public boolean incrementToken() {\n            if (!iter.hasNext())\n                return false;\n            Object obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            termAtt.setTermBuffer(term);\n            offsetAtt.setOffset(start, start + termAtt.termLength());\n            start +\u003d term.length() + 1;\n            return true;\n        }\n    };\n}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 271,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "@@ -1,23 +1,27 @@\n public TokenStream keywordTokenStream(final Collection keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n         private Iterator iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n-        public Token next(final Token reusableToken) {\n-            assert reusableToken !\u003d null;\n+        private TermAttribute termAtt \u003d (TermAttribute) addAttribute(TermAttribute.class);\n+\n+        private OffsetAttribute offsetAtt \u003d (OffsetAttribute) addAttribute(OffsetAttribute.class);\n+\n+        public boolean incrementToken() {\n             if (!iter.hasNext())\n-                return null;\n+                return false;\n             Object obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n-            reusableToken.reinit(term, start, start + reusableToken.termLength());\n+            termAtt.setTermBuffer(term);\n+            offsetAtt.setOffset(start, start + termAtt.termLength());\n             start +\u003d term.length() + 1;\n-            return reusableToken;\n+            return true;\n         }\n     };\n }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "bb6b7117186b656b4777850fdc463e0eaa541130": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-1333: improvements to Token reuse API and full cutover to reuse API for all core and contrib analyzers\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@687357 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2008/8/20 下午10:38",
      "commitName": "bb6b7117186b656b4777850fdc463e0eaa541130",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2008/8/19 下午6:40",
      "commitNameOld": "e31a9da835c6e381bbcd46aa4390089f6bcc1c15",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "actualSource": "public TokenStream keywordTokenStream(final Collection keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        public Token next(final Token reusableToken) {\n            assert reusableToken !\u003d null;\n            if (!iter.hasNext())\n                return null;\n            Object obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            reusableToken.reinit(term, start, start + reusableToken.termLength());\n            start +\u003d term.length() + 1;\n            return reusableToken;\n        }\n    };\n}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 270,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n",
      "diff": "@@ -1,22 +1,23 @@\n public TokenStream keywordTokenStream(final Collection keywords) {\n     if (keywords \u003d\u003d null)\n         throw new IllegalArgumentException(\"keywords must not be null\");\n     return new TokenStream() {\n \n         private Iterator iter \u003d keywords.iterator();\n \n         private int start \u003d 0;\n \n-        public Token next() {\n+        public Token next(final Token reusableToken) {\n+            assert reusableToken !\u003d null;\n             if (!iter.hasNext())\n                 return null;\n             Object obj \u003d iter.next();\n             if (obj \u003d\u003d null)\n                 throw new IllegalArgumentException(\"keyword must not be null\");\n             String term \u003d obj.toString();\n-            Token token \u003d new Token(term, start, start + term.length());\n+            reusableToken.reinit(term, start, start + reusableToken.termLength());\n             start +\u003d term.length() + 1;\n-            return token;\n+            return reusableToken;\n         }\n     };\n }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "e28541354d496a43078c1bc281076f97ed7d008c": {
      "type": "Yintroduced",
      "commitMessage": "some performance improvements\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@351891 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2005/12/3 下午1:24",
      "commitName": "e28541354d496a43078c1bc281076f97ed7d008c",
      "commitAuthor": "Wolfgang Hoschek",
      "diff": "@@ -0,0 +1,22 @@\n+public TokenStream keywordTokenStream(final Collection keywords) {\n+    if (keywords \u003d\u003d null)\n+        throw new IllegalArgumentException(\"keywords must not be null\");\n+    return new TokenStream() {\n+\n+        private Iterator iter \u003d keywords.iterator();\n+\n+        private int start \u003d 0;\n+\n+        public Token next() {\n+            if (!iter.hasNext())\n+                return null;\n+            Object obj \u003d iter.next();\n+            if (obj \u003d\u003d null)\n+                throw new IllegalArgumentException(\"keyword must not be null\");\n+            String term \u003d obj.toString();\n+            Token token \u003d new Token(term, start, start + term.length());\n+            start +\u003d term.length() + 1;\n+            return token;\n+        }\n+    };\n+}\n\\ No newline at end of file\n",
      "actualSource": "public TokenStream keywordTokenStream(final Collection keywords) {\n    if (keywords \u003d\u003d null)\n        throw new IllegalArgumentException(\"keywords must not be null\");\n    return new TokenStream() {\n\n        private Iterator iter \u003d keywords.iterator();\n\n        private int start \u003d 0;\n\n        public Token next() {\n            if (!iter.hasNext())\n                return null;\n            Object obj \u003d iter.next();\n            if (obj \u003d\u003d null)\n                throw new IllegalArgumentException(\"keyword must not be null\");\n            String term \u003d obj.toString();\n            Token token \u003d new Token(term, start, start + term.length());\n            start +\u003d term.length() + 1;\n            return token;\n        }\n    };\n}",
      "path": "contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java",
      "functionStartLine": 252,
      "functionName": "keywordTokenStream",
      "functionAnnotation": "",
      "functionDoc": "Convenience method; Creates and returns a token stream that generates a\ntoken for each keyword in the given collection, \"as is\", without any\ntransforming text analysis. The resulting token stream can be fed into\n{@link #addField(String, TokenStream)}, perhaps wrapped into another\n{@link org.apache.lucene.analysis.TokenFilter}, as desired.\n\n@param keywords the keywords to generate tokens for\n@return the corresponding token stream\n"
    }
  }
}