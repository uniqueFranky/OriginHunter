{
  "origin": "codeshovel",
  "repositoryName": "scikit-learn",
  "repositoryPath": "/Users/franky/Documents/Homework/毕业设计/testcase/scikit-learn/.git",
  "startCommitName": "db59dd74df576345cc026cc0d6c99392d3649d3b",
  "sourceFileName": "pipeline.py",
  "functionName": "_fit",
  "functionId": "Pipeline___fit___self__X__y(default-None)__**fit_params",
  "sourceFilePath": "sklearn/pipeline.py",
  "functionAnnotation": "",
  "functionStartLine": 260,
  "functionEndLine": 321,
  "numCommitsSeen": 124,
  "timeTaken": 7168,
  "changeHistory": [
    "13981bdce97ab2dd49b6b8707f3f27b5c148b9c0",
    "8edd9f9f1597094a1f8024f4d091f21e9c1fa66a",
    "0e54f44b4e26b28819c1e745793fe5bec9b1cea9",
    "0e3bb17e6263a391c8409558634b75f437eb3f27",
    "952ef6637a13ba01055bf0ed9d29d525f0fee5bc",
    "1b90237b45dfa1652831cfe72bd4b29a31b070ec",
    "9f842e3496ab59ad593b46553fadff7b2490ef1a",
    "47d3b3c30fd408187ce0524043be43bf378397dd",
    "0b6308c2708fe03071cdbf24997eb967403f5965",
    "a65a9e53af4873729ee98de76f0e8afa6f39d8e5",
    "e66aa6dc70bb43541bd2a084ea28d0547b89ed4f",
    "4c33ea2ee0587f47da3b7e2162ef29fff0ee7f93",
    "b3a639ffc2d518b8862c61e4170403a400368571",
    "5b20d484add50aec64a1bda5c52ed2ceb7557f36"
  ],
  "changeHistoryShort": {
    "13981bdce97ab2dd49b6b8707f3f27b5c148b9c0": "Ybodychange",
    "8edd9f9f1597094a1f8024f4d091f21e9c1fa66a": "Ybodychange",
    "0e54f44b4e26b28819c1e745793fe5bec9b1cea9": "Ybodychange",
    "0e3bb17e6263a391c8409558634b75f437eb3f27": "Ybodychange",
    "952ef6637a13ba01055bf0ed9d29d525f0fee5bc": "Ybodychange",
    "1b90237b45dfa1652831cfe72bd4b29a31b070ec": "Ybodychange",
    "9f842e3496ab59ad593b46553fadff7b2490ef1a": "Ybodychange",
    "47d3b3c30fd408187ce0524043be43bf378397dd": "Ybodychange",
    "0b6308c2708fe03071cdbf24997eb967403f5965": "Ybodychange",
    "a65a9e53af4873729ee98de76f0e8afa6f39d8e5": "Ybodychange",
    "e66aa6dc70bb43541bd2a084ea28d0547b89ed4f": "Ybodychange",
    "4c33ea2ee0587f47da3b7e2162ef29fff0ee7f93": "Ybodychange",
    "b3a639ffc2d518b8862c61e4170403a400368571": "Ybodychange",
    "5b20d484add50aec64a1bda5c52ed2ceb7557f36": "Yintroduced"
  },
  "changeHistoryDetails": {
    "13981bdce97ab2dd49b6b8707f3f27b5c148b9c0": {
      "type": "Ybodychange",
      "commitMessage": "STY Remove variable renaming (#13731)\n\n",
      "commitDate": "2019/5/5 下午11:27",
      "commitName": "13981bdce97ab2dd49b6b8707f3f27b5c148b9c0",
      "commitAuthor": "Martin Thoma",
      "commitDateOld": "2019/5/1 下午11:29",
      "commitNameOld": "e7ca6236c50f94777343d8a7a9e8af73b6c24276",
      "commitAuthorOld": "Stephen Cowley",
      "daysBetweenCommits": 4.0,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            if \u0027__\u0027 not in pname:\n                raise ValueError(\n                    \"Pipeline.fit does not accept the {} parameter. \"\n                    \"You can pass parameters to specific steps of your \"\n                    \"pipeline using the stepname__parameter format, e.g. \"\n                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                    \"\u003dsample_weight)`.\".format(pname))\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        for (step_idx,\n             name,\n             transformer) in self._iter(with_final\u003dFalse,\n                                        filter_passthrough\u003dFalse):\n            if (transformer is None or transformer \u003d\u003d \u0027passthrough\u0027):\n                with _print_elapsed_time(\u0027Pipeline\u0027,\n                                         self._log_message(step_idx)):\n                    continue\n\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            X, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, X, y, None,\n                message_clsname\u003d\u0027Pipeline\u0027,\n                message\u003dself._log_message(step_idx),\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return X, {}\n        return X, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 263,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,62 +1,61 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d {name: {} for name, step in self.steps\n                             if step is not None}\n         for pname, pval in fit_params.items():\n             if \u0027__\u0027 not in pname:\n                 raise ValueError(\n                     \"Pipeline.fit does not accept the {} parameter. \"\n                     \"You can pass parameters to specific steps of your \"\n                     \"pipeline using the stepname__parameter format, e.g. \"\n                     \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                     \"\u003dsample_weight)`.\".format(pname))\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n-        Xt \u003d X\n         for (step_idx,\n              name,\n              transformer) in self._iter(with_final\u003dFalse,\n                                         filter_passthrough\u003dFalse):\n             if (transformer is None or transformer \u003d\u003d \u0027passthrough\u0027):\n                 with _print_elapsed_time(\u0027Pipeline\u0027,\n                                          self._log_message(step_idx)):\n                     continue\n \n             if hasattr(memory, \u0027location\u0027):\n                 # joblib \u003e\u003d 0.12\n                 if memory.location is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             elif hasattr(memory, \u0027cachedir\u0027):\n                 # joblib \u003c 0.11\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             else:\n                 cloned_transformer \u003d clone(transformer)\n             # Fit or load from cache the current transfomer\n-            Xt, fitted_transformer \u003d fit_transform_one_cached(\n-                cloned_transformer, Xt, y, None,\n+            X, fitted_transformer \u003d fit_transform_one_cached(\n+                cloned_transformer, X, y, None,\n                 message_clsname\u003d\u0027Pipeline\u0027,\n                 message\u003dself._log_message(step_idx),\n                 **fit_params_steps[name])\n             # Replace the transformer of the step with the fitted\n             # transformer. This is necessary when loading the transformer\n             # from the cache.\n             self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n-            return Xt, {}\n-        return Xt, fit_params_steps[self.steps[-1][0]]\n+            return X, {}\n+        return X, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "8edd9f9f1597094a1f8024f4d091f21e9c1fa66a": {
      "type": "Ybodychange",
      "commitMessage": "ENH Add verbose option to Pipeline, FeatureUnion, and ColumnTransformer (#11364)\n\n",
      "commitDate": "2019/4/21 下午10:51",
      "commitName": "8edd9f9f1597094a1f8024f4d091f21e9c1fa66a",
      "commitAuthor": "Thomas J Fan",
      "commitDateOld": "2019/4/9 下午6:54",
      "commitNameOld": "0e54f44b4e26b28819c1e745793fe5bec9b1cea9",
      "commitAuthorOld": "Osman",
      "daysBetweenCommits": 12.16,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            if \u0027__\u0027 not in pname:\n                raise ValueError(\n                    \"Pipeline.fit does not accept the {} parameter. \"\n                    \"You can pass parameters to specific steps of your \"\n                    \"pipeline using the stepname__parameter format, e.g. \"\n                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                    \"\u003dsample_weight)`.\".format(pname))\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for (step_idx,\n             name,\n             transformer) in self._iter(with_final\u003dFalse,\n                                        filter_passthrough\u003dFalse):\n            if (transformer is None or transformer \u003d\u003d \u0027passthrough\u0027):\n                with _print_elapsed_time(\u0027Pipeline\u0027,\n                                         self._log_message(step_idx)):\n                    continue\n\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                message_clsname\u003d\u0027Pipeline\u0027,\n                message\u003dself._log_message(step_idx),\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 263,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,52 +1,62 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d {name: {} for name, step in self.steps\n                             if step is not None}\n         for pname, pval in fit_params.items():\n             if \u0027__\u0027 not in pname:\n                 raise ValueError(\n                     \"Pipeline.fit does not accept the {} parameter. \"\n                     \"You can pass parameters to specific steps of your \"\n                     \"pipeline using the stepname__parameter format, e.g. \"\n                     \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                     \"\u003dsample_weight)`.\".format(pname))\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n-        for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n+        for (step_idx,\n+             name,\n+             transformer) in self._iter(with_final\u003dFalse,\n+                                        filter_passthrough\u003dFalse):\n+            if (transformer is None or transformer \u003d\u003d \u0027passthrough\u0027):\n+                with _print_elapsed_time(\u0027Pipeline\u0027,\n+                                         self._log_message(step_idx)):\n+                    continue\n+\n             if hasattr(memory, \u0027location\u0027):\n                 # joblib \u003e\u003d 0.12\n                 if memory.location is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             elif hasattr(memory, \u0027cachedir\u0027):\n                 # joblib \u003c 0.11\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             else:\n                 cloned_transformer \u003d clone(transformer)\n             # Fit or load from cache the current transfomer\n             Xt, fitted_transformer \u003d fit_transform_one_cached(\n                 cloned_transformer, Xt, y, None,\n+                message_clsname\u003d\u0027Pipeline\u0027,\n+                message\u003dself._log_message(step_idx),\n                 **fit_params_steps[name])\n             # Replace the transformer of the step with the fitted\n             # transformer. This is necessary when loading the transformer\n             # from the cache.\n             self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "0e54f44b4e26b28819c1e745793fe5bec9b1cea9": {
      "type": "Ybodychange",
      "commitMessage": "Improve pipeline parameter error msg (#13536)\n\n",
      "commitDate": "2019/4/9 下午6:54",
      "commitName": "0e54f44b4e26b28819c1e745793fe5bec9b1cea9",
      "commitAuthor": "Osman",
      "commitDateOld": "2019/3/16 下午11:34",
      "commitNameOld": "3a2fbe1d348e1eb8e0ea67edee44ead35b2240c9",
      "commitAuthorOld": "Lakshya KD",
      "daysBetweenCommits": 23.81,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            if \u0027__\u0027 not in pname:\n                raise ValueError(\n                    \"Pipeline.fit does not accept the {} parameter. \"\n                    \"You can pass parameters to specific steps of your \"\n                    \"pipeline using the stepname__parameter format, e.g. \"\n                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n                    \"\u003dsample_weight)`.\".format(pname))\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 244,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,45 +1,52 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d {name: {} for name, step in self.steps\n                             if step is not None}\n         for pname, pval in fit_params.items():\n+            if \u0027__\u0027 not in pname:\n+                raise ValueError(\n+                    \"Pipeline.fit does not accept the {} parameter. \"\n+                    \"You can pass parameters to specific steps of your \"\n+                    \"pipeline using the stepname__parameter format, e.g. \"\n+                    \"`Pipeline.fit(X, y, logisticregression__sample_weight\"\n+                    \"\u003dsample_weight)`.\".format(pname))\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n             if hasattr(memory, \u0027location\u0027):\n                 # joblib \u003e\u003d 0.12\n                 if memory.location is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             elif hasattr(memory, \u0027cachedir\u0027):\n                 # joblib \u003c 0.11\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             else:\n                 cloned_transformer \u003d clone(transformer)\n             # Fit or load from cache the current transfomer\n             Xt, fitted_transformer \u003d fit_transform_one_cached(\n                 cloned_transformer, Xt, y, None,\n                 **fit_params_steps[name])\n             # Replace the transformer of the step with the fitted\n             # transformer. This is necessary when loading the transformer\n             # from the cache.\n             self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "0e3bb17e6263a391c8409558634b75f437eb3f27": {
      "type": "Ybodychange",
      "commitMessage": "MAINT Run pyupgrade following Python2 deprecation (#12997)\n\n",
      "commitDate": "2019/2/8 下午8:13",
      "commitName": "0e3bb17e6263a391c8409558634b75f437eb3f27",
      "commitAuthor": "Roman Yurchak",
      "commitDateOld": "2019/1/3 下午9:50",
      "commitNameOld": "952ef6637a13ba01055bf0ed9d29d525f0fee5bc",
      "commitAuthorOld": "Andreas Mueller",
      "daysBetweenCommits": 35.93,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d {name: {} for name, step in self.steps\n                            if step is not None}\n        for pname, pval in fit_params.items():\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 206,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,45 +1,45 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n-        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n-                                if step is not None)\n+        fit_params_steps \u003d {name: {} for name, step in self.steps\n+                            if step is not None}\n         for pname, pval in fit_params.items():\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n             if hasattr(memory, \u0027location\u0027):\n                 # joblib \u003e\u003d 0.12\n                 if memory.location is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             elif hasattr(memory, \u0027cachedir\u0027):\n                 # joblib \u003c 0.11\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             else:\n                 cloned_transformer \u003d clone(transformer)\n             # Fit or load from cache the current transfomer\n             Xt, fitted_transformer \u003d fit_transform_one_cached(\n                 cloned_transformer, Xt, y, None,\n                 **fit_params_steps[name])\n             # Replace the transformer of the step with the fitted\n             # transformer. This is necessary when loading the transformer\n             # from the cache.\n             self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "952ef6637a13ba01055bf0ed9d29d525f0fee5bc": {
      "type": "Ybodychange",
      "commitMessage": "MRG Drop legacy python / remove six dependencies (#12639)\n\n",
      "commitDate": "2019/1/3 下午9:50",
      "commitName": "952ef6637a13ba01055bf0ed9d29d525f0fee5bc",
      "commitAuthor": "Andreas Mueller",
      "commitDateOld": "2018/11/26 下午9:39",
      "commitNameOld": "1b90237b45dfa1652831cfe72bd4b29a31b070ec",
      "commitAuthorOld": "Thomas Fan",
      "daysBetweenCommits": 38.01,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in fit_params.items():\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 206,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,45 +1,45 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n-        for pname, pval in six.iteritems(fit_params):\n+        for pname, pval in fit_params.items():\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n             if hasattr(memory, \u0027location\u0027):\n                 # joblib \u003e\u003d 0.12\n                 if memory.location is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             elif hasattr(memory, \u0027cachedir\u0027):\n                 # joblib \u003c 0.11\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             else:\n                 cloned_transformer \u003d clone(transformer)\n             # Fit or load from cache the current transfomer\n             Xt, fitted_transformer \u003d fit_transform_one_cached(\n                 cloned_transformer, Xt, y, None,\n                 **fit_params_steps[name])\n             # Replace the transformer of the step with the fitted\n             # transformer. This is necessary when loading the transformer\n             # from the cache.\n             self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "1b90237b45dfa1652831cfe72bd4b29a31b070ec": {
      "type": "Ybodychange",
      "commitMessage": "FIX Pipeline steps incorrectly updated with passthrough step (#12659)\n\n",
      "commitDate": "2018/11/26 下午9:39",
      "commitName": "1b90237b45dfa1652831cfe72bd4b29a31b070ec",
      "commitAuthor": "Thomas Fan",
      "commitDateOld": "2018/11/20 上午7:53",
      "commitNameOld": "d25da1be2054170034548b63ce87bd459ed8bee0",
      "commitAuthorOld": "Thomas Moreau",
      "daysBetweenCommits": 6.57,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 207,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,46 +1,45 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n-        for step_idx, (name, transformer) in enumerate(\n-                self._iter(with_final\u003dFalse)):\n+        for step_idx, name, transformer in self._iter(with_final\u003dFalse):\n             if hasattr(memory, \u0027location\u0027):\n                 # joblib \u003e\u003d 0.12\n                 if memory.location is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             elif hasattr(memory, \u0027cachedir\u0027):\n                 # joblib \u003c 0.11\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to\n                     # preserve backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n             else:\n                 cloned_transformer \u003d clone(transformer)\n             # Fit or load from cache the current transfomer\n             Xt, fitted_transformer \u003d fit_transform_one_cached(\n                 cloned_transformer, Xt, y, None,\n                 **fit_params_steps[name])\n             # Replace the transformer of the step with the fitted\n             # transformer. This is necessary when loading the transformer\n             # from the cache.\n             self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "9f842e3496ab59ad593b46553fadff7b2490ef1a": {
      "type": "Ybodychange",
      "commitMessage": "API Adds passthrough option to Pipeline (#11674)\n\n",
      "commitDate": "2018/11/5 上午10:21",
      "commitName": "9f842e3496ab59ad593b46553fadff7b2490ef1a",
      "commitAuthor": "Thomas Fan",
      "commitDateOld": "2018/9/13 下午11:20",
      "commitNameOld": "5be05c64d9bd222a9ff82e963e0ae80cc2aeefdb",
      "commitAuthorOld": "Thomas Fan",
      "daysBetweenCommits": 52.46,
      "commitsBetweenForRepo": 165,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(\n                self._iter(with_final\u003dFalse)):\n            if hasattr(memory, \u0027location\u0027):\n                # joblib \u003e\u003d 0.12\n                if memory.location is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            elif hasattr(memory, \u0027cachedir\u0027):\n                # joblib \u003c 0.11\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to\n                    # preserve backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n            else:\n                cloned_transformer \u003d clone(transformer)\n            # Fit or load from cache the current transfomer\n            Xt, fitted_transformer \u003d fit_transform_one_cached(\n                cloned_transformer, Xt, y, None,\n                **fit_params_steps[name])\n            # Replace the transformer of the step with the fitted\n            # transformer. This is necessary when loading the transformer\n            # from the cache.\n            self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 207,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,48 +1,46 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n-        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n-            if transformer is None:\n-                pass\n-            else:\n-                if hasattr(memory, \u0027location\u0027):\n-                    # joblib \u003e\u003d 0.12\n-                    if memory.location is None:\n-                        # we do not clone when caching is disabled to\n-                        # preserve backward compatibility\n-                        cloned_transformer \u003d transformer\n-                    else:\n-                        cloned_transformer \u003d clone(transformer)\n-                elif hasattr(memory, \u0027cachedir\u0027):\n-                    # joblib \u003c 0.11\n-                    if memory.cachedir is None:\n-                        # we do not clone when caching is disabled to\n-                        # preserve backward compatibility\n-                        cloned_transformer \u003d transformer\n-                    else:\n-                        cloned_transformer \u003d clone(transformer)\n+        for step_idx, (name, transformer) in enumerate(\n+                self._iter(with_final\u003dFalse)):\n+            if hasattr(memory, \u0027location\u0027):\n+                # joblib \u003e\u003d 0.12\n+                if memory.location is None:\n+                    # we do not clone when caching is disabled to\n+                    # preserve backward compatibility\n+                    cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n-                # Fit or load from cache the current transfomer\n-                Xt, fitted_transformer \u003d fit_transform_one_cached(\n-                    cloned_transformer, Xt, y, None,\n-                    **fit_params_steps[name])\n-                # Replace the transformer of the step with the fitted\n-                # transformer. This is necessary when loading the transformer\n-                # from the cache.\n-                self.steps[step_idx] \u003d (name, fitted_transformer)\n-        if self._final_estimator is None:\n+            elif hasattr(memory, \u0027cachedir\u0027):\n+                # joblib \u003c 0.11\n+                if memory.cachedir is None:\n+                    # we do not clone when caching is disabled to\n+                    # preserve backward compatibility\n+                    cloned_transformer \u003d transformer\n+                else:\n+                    cloned_transformer \u003d clone(transformer)\n+            else:\n+                cloned_transformer \u003d clone(transformer)\n+            # Fit or load from cache the current transfomer\n+            Xt, fitted_transformer \u003d fit_transform_one_cached(\n+                cloned_transformer, Xt, y, None,\n+                **fit_params_steps[name])\n+            # Replace the transformer of the step with the fitted\n+            # transformer. This is necessary when loading the transformer\n+            # from the cache.\n+            self.steps[step_idx] \u003d (name, fitted_transformer)\n+        if self._final_estimator \u003d\u003d \u0027passthrough\u0027:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "47d3b3c30fd408187ce0524043be43bf378397dd": {
      "type": "Ybodychange",
      "commitMessage": "[MRG] TST: avoid DeprecationWarning on latest joblib (#11625)\n\n",
      "commitDate": "2018/7/20 下午10:13",
      "commitName": "47d3b3c30fd408187ce0524043be43bf378397dd",
      "commitAuthor": "Gael Varoquaux",
      "commitDateOld": "2018/7/18 上午12:02",
      "commitNameOld": "14e7c328df9968ca403e47168314ae28390c07d5",
      "commitAuthorOld": "Joel Nothman",
      "daysBetweenCommits": 2.92,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, \u0027location\u0027):\n                    # joblib \u003e\u003d 0.12\n                    if memory.location is None:\n                        # we do not clone when caching is disabled to\n                        # preserve backward compatibility\n                        cloned_transformer \u003d transformer\n                    else:\n                        cloned_transformer \u003d clone(transformer)\n                elif hasattr(memory, \u0027cachedir\u0027):\n                    # joblib \u003c 0.11\n                    if memory.cachedir is None:\n                        # we do not clone when caching is disabled to\n                        # preserve backward compatibility\n                        cloned_transformer \u003d transformer\n                    else:\n                        cloned_transformer \u003d clone(transformer)\n                else:\n                    cloned_transformer \u003d clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer \u003d fit_transform_one_cached(\n                    cloned_transformer, Xt, y, None,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 190,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,36 +1,48 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n             if transformer is None:\n                 pass\n             else:\n-                if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n-                    # we do not clone when caching is disabled to preserve\n-                    # backward compatibility\n-                    cloned_transformer \u003d transformer\n+                if hasattr(memory, \u0027location\u0027):\n+                    # joblib \u003e\u003d 0.12\n+                    if memory.location is None:\n+                        # we do not clone when caching is disabled to\n+                        # preserve backward compatibility\n+                        cloned_transformer \u003d transformer\n+                    else:\n+                        cloned_transformer \u003d clone(transformer)\n+                elif hasattr(memory, \u0027cachedir\u0027):\n+                    # joblib \u003c 0.11\n+                    if memory.cachedir is None:\n+                        # we do not clone when caching is disabled to\n+                        # preserve backward compatibility\n+                        cloned_transformer \u003d transformer\n+                    else:\n+                        cloned_transformer \u003d clone(transformer)\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n                 # Fit or load from cache the current transfomer\n                 Xt, fitted_transformer \u003d fit_transform_one_cached(\n                     cloned_transformer, Xt, y, None,\n                     **fit_params_steps[name])\n                 # Replace the transformer of the step with the fitted\n                 # transformer. This is necessary when loading the transformer\n                 # from the cache.\n                 self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator is None:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "0b6308c2708fe03071cdbf24997eb967403f5965": {
      "type": "Ybodychange",
      "commitMessage": "FEA Add ColumnTransformer for heterogeneous data (#9012)\n\n",
      "commitDate": "2018/5/30 上午5:49",
      "commitName": "0b6308c2708fe03071cdbf24997eb967403f5965",
      "commitAuthor": "Joris Van den Bossche",
      "commitDateOld": "2018/3/27 下午1:44",
      "commitNameOld": "20661b5018ba45f4452218a689640ff007cd361f",
      "commitAuthorOld": "Loïc Estève",
      "daysBetweenCommits": 63.67,
      "commitsBetweenForRepo": 105,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer \u003d fit_transform_one_cached(\n                    cloned_transformer, Xt, y, None,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 185,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,36 +1,36 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         # shallow copy of steps - this should really be steps_\n         self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n             if transformer is None:\n                 pass\n             else:\n                 if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n                     # we do not clone when caching is disabled to preserve\n                     # backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n                 # Fit or load from cache the current transfomer\n                 Xt, fitted_transformer \u003d fit_transform_one_cached(\n-                    cloned_transformer, None, Xt, y,\n+                    cloned_transformer, Xt, y, None,\n                     **fit_params_steps[name])\n                 # Replace the transformer of the step with the fitted\n                 # transformer. This is necessary when loading the transformer\n                 # from the cache.\n                 self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator is None:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "a65a9e53af4873729ee98de76f0e8afa6f39d8e5": {
      "type": "Ybodychange",
      "commitMessage": "[MRG+1] Don\u0027t modify steps in {Pipeline,FeatureUnion}.__init__ (#9716)\n\n",
      "commitDate": "2017/9/13 上午7:20",
      "commitName": "a65a9e53af4873729ee98de76f0e8afa6f39d8e5",
      "commitAuthor": "Andreas Mueller",
      "commitDateOld": "2017/8/31 下午4:28",
      "commitNameOld": "4889a67942713777e0e250eda9a3e019d84d1950",
      "commitAuthorOld": "Loïc Estève",
      "daysBetweenCommits": 12.62,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        # shallow copy of steps - this should really be steps_\n        self.steps \u003d list(self.steps)\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer \u003d fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 185,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,34 +1,36 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n+        # shallow copy of steps - this should really be steps_\n+        self.steps \u003d list(self.steps)\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n             if transformer is None:\n                 pass\n             else:\n                 if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n                     # we do not clone when caching is disabled to preserve\n                     # backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n                 # Fit or load from cache the current transfomer\n                 Xt, fitted_transformer \u003d fit_transform_one_cached(\n                     cloned_transformer, None, Xt, y,\n                     **fit_params_steps[name])\n                 # Replace the transformer of the step with the fitted\n                 # transformer. This is necessary when loading the transformer\n                 # from the cache.\n                 self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator is None:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "e66aa6dc70bb43541bd2a084ea28d0547b89ed4f": {
      "type": "Ybodychange",
      "commitMessage": "ENH Ducktyping to allow for alternative Memory implementations (#9584)\n\n",
      "commitDate": "2017/8/30 下午1:21",
      "commitName": "e66aa6dc70bb43541bd2a084ea28d0547b89ed4f",
      "commitAuthor": "Kumar Ashutosh",
      "commitDateOld": "2017/8/23 上午7:56",
      "commitNameOld": "aae87002b96622424a16dcad2eaef3a75cc0feda",
      "commitAuthorOld": "Joris Van den Bossche",
      "daysBetweenCommits": 7.23,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d check_memory(self.memory)\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer \u003d fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 186,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,43 +1,34 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         self._validate_steps()\n         # Setup the memory\n-        memory \u003d self.memory\n-        if memory is None:\n-            memory \u003d Memory(cachedir\u003dNone, verbose\u003d0)\n-        elif isinstance(memory, six.string_types):\n-            memory \u003d Memory(cachedir\u003dmemory, verbose\u003d0)\n-        elif not isinstance(memory, Memory):\n-            raise ValueError(\"\u0027memory\u0027 should either be a string or\"\n-                             \" a sklearn.externals.joblib.Memory\"\n-                             \" instance, got \u0027memory\u003d{!r}\u0027 instead.\".format(\n-                                 type(memory)))\n+        memory \u003d check_memory(self.memory)\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n             if transformer is None:\n                 pass\n             else:\n-                if memory.cachedir is None:\n+                if hasattr(memory, \u0027cachedir\u0027) and memory.cachedir is None:\n                     # we do not clone when caching is disabled to preserve\n                     # backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n                 # Fit or load from cache the current transfomer\n                 Xt, fitted_transformer \u003d fit_transform_one_cached(\n                     cloned_transformer, None, Xt, y,\n                     **fit_params_steps[name])\n                 # Replace the transformer of the step with the fitted\n                 # transformer. This is necessary when loading the transformer\n                 # from the cache.\n                 self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator is None:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "4c33ea2ee0587f47da3b7e2162ef29fff0ee7f93": {
      "type": "Ybodychange",
      "commitMessage": "[MRG+1] EHN/DOC Make error msg about Memory more explicit (#8865)\n\n",
      "commitDate": "2017/5/17 下午8:51",
      "commitName": "4c33ea2ee0587f47da3b7e2162ef29fff0ee7f93",
      "commitAuthor": "Guillaume Lemaitre",
      "commitDateOld": "2017/5/8 上午6:08",
      "commitNameOld": "6f74a7cfa6dc0c1149c129011c85d121f4faee23",
      "commitAuthorOld": "Guillaume Lemaitre",
      "daysBetweenCommits": 9.61,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d self.memory\n        if memory is None:\n            memory \u003d Memory(cachedir\u003dNone, verbose\u003d0)\n        elif isinstance(memory, six.string_types):\n            memory \u003d Memory(cachedir\u003dmemory, verbose\u003d0)\n        elif not isinstance(memory, Memory):\n            raise ValueError(\"\u0027memory\u0027 should either be a string or\"\n                             \" a sklearn.externals.joblib.Memory\"\n                             \" instance, got \u0027memory\u003d{!r}\u0027 instead.\".format(\n                                 type(memory)))\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer \u003d fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 187,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,42 +1,43 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         self._validate_steps()\n         # Setup the memory\n         memory \u003d self.memory\n         if memory is None:\n             memory \u003d Memory(cachedir\u003dNone, verbose\u003d0)\n         elif isinstance(memory, six.string_types):\n             memory \u003d Memory(cachedir\u003dmemory, verbose\u003d0)\n         elif not isinstance(memory, Memory):\n             raise ValueError(\"\u0027memory\u0027 should either be a string or\"\n-                             \" a joblib.Memory instance, got\"\n-                             \" \u0027memory\u003d{!r}\u0027 instead.\".format(memory))\n+                             \" a sklearn.externals.joblib.Memory\"\n+                             \" instance, got \u0027memory\u003d{!r}\u0027 instead.\".format(\n+                                 type(memory)))\n \n         fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n \n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n         for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n             if transformer is None:\n                 pass\n             else:\n                 if memory.cachedir is None:\n                     # we do not clone when caching is disabled to preserve\n                     # backward compatibility\n                     cloned_transformer \u003d transformer\n                 else:\n                     cloned_transformer \u003d clone(transformer)\n                 # Fit or load from cache the current transfomer\n                 Xt, fitted_transformer \u003d fit_transform_one_cached(\n                     cloned_transformer, None, Xt, y,\n                     **fit_params_steps[name])\n                 # Replace the transformer of the step with the fitted\n                 # transformer. This is necessary when loading the transformer\n                 # from the cache.\n                 self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator is None:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "b3a639ffc2d518b8862c61e4170403a400368571": {
      "type": "Ybodychange",
      "commitMessage": "[MRG+3] ENH Caching Pipeline by memoizing transformer (#7990)\n\n* ENH Caching Pipeline by memoizing transformer\r\n\r\n* Fix lesteve changes\r\n\r\n* Fix comments\r\n\r\n* Fix doc\r\n\r\n* Fix jnothman comments\r\n",
      "commitDate": "2017/2/13 下午9:01",
      "commitName": "b3a639ffc2d518b8862c61e4170403a400368571",
      "commitAuthor": "Guillaume Lemaitre",
      "commitDateOld": "2016/12/16 上午5:04",
      "commitNameOld": "8056d6355e015138eea07bee57e5a3d8e2148802",
      "commitAuthorOld": "alexandercbooth",
      "daysBetweenCommits": 59.66,
      "commitsBetweenForRepo": 100,
      "commitsBetweenForFile": 1,
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        self._validate_steps()\n        # Setup the memory\n        memory \u003d self.memory\n        if memory is None:\n            memory \u003d Memory(cachedir\u003dNone, verbose\u003d0)\n        elif isinstance(memory, six.string_types):\n            memory \u003d Memory(cachedir\u003dmemory, verbose\u003d0)\n        elif not isinstance(memory, Memory):\n            raise ValueError(\"\u0027memory\u0027 should either be a string or\"\n                             \" a joblib.Memory instance, got\"\n                             \" \u0027memory\u003d{!r}\u0027 instead.\".format(memory))\n\n        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n            if transformer is None:\n                pass\n            else:\n                if memory.cachedir is None:\n                    # we do not clone when caching is disabled to preserve\n                    # backward compatibility\n                    cloned_transformer \u003d transformer\n                else:\n                    cloned_transformer \u003d clone(transformer)\n                # Fit or load from cache the current transfomer\n                Xt, fitted_transformer \u003d fit_transform_one_cached(\n                    cloned_transformer, None, Xt, y,\n                    **fit_params_steps[name])\n                # Replace the transformer of the step with the fitted\n                # transformer. This is necessary when loading the transformer\n                # from the cache.\n                self.steps[step_idx] \u003d (name, fitted_transformer)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 238,
      "functionName": "_fit",
      "functionAnnotation": "",
      "diff": "@@ -1,19 +1,42 @@\n     def _fit(self, X, y\u003dNone, **fit_params):\n         self._validate_steps()\n+        # Setup the memory\n+        memory \u003d self.memory\n+        if memory is None:\n+            memory \u003d Memory(cachedir\u003dNone, verbose\u003d0)\n+        elif isinstance(memory, six.string_types):\n+            memory \u003d Memory(cachedir\u003dmemory, verbose\u003d0)\n+        elif not isinstance(memory, Memory):\n+            raise ValueError(\"\u0027memory\u0027 should either be a string or\"\n+                             \" a joblib.Memory instance, got\"\n+                             \" \u0027memory\u003d{!r}\u0027 instead.\".format(memory))\n+\n+        fit_transform_one_cached \u003d memory.cache(_fit_transform_one)\n+\n         fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                 if step is not None)\n         for pname, pval in six.iteritems(fit_params):\n             step, param \u003d pname.split(\u0027__\u0027, 1)\n             fit_params_steps[step][param] \u003d pval\n         Xt \u003d X\n-        for name, transform in self.steps[:-1]:\n-            if transform is None:\n+        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):\n+            if transformer is None:\n                 pass\n-            elif hasattr(transform, \"fit_transform\"):\n-                Xt \u003d transform.fit_transform(Xt, y, **fit_params_steps[name])\n             else:\n-                Xt \u003d transform.fit(Xt, y, **fit_params_steps[name]) \\\n-                              .transform(Xt)\n+                if memory.cachedir is None:\n+                    # we do not clone when caching is disabled to preserve\n+                    # backward compatibility\n+                    cloned_transformer \u003d transformer\n+                else:\n+                    cloned_transformer \u003d clone(transformer)\n+                # Fit or load from cache the current transfomer\n+                Xt, fitted_transformer \u003d fit_transform_one_cached(\n+                    cloned_transformer, None, Xt, y,\n+                    **fit_params_steps[name])\n+                # Replace the transformer of the step with the fitted\n+                # transformer. This is necessary when loading the transformer\n+                # from the cache.\n+                self.steps[step_idx] \u003d (name, fitted_transformer)\n         if self._final_estimator is None:\n             return Xt, {}\n         return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "extendedDetails": {}
    },
    "5b20d484add50aec64a1bda5c52ed2ceb7557f36": {
      "type": "Yintroduced",
      "commitMessage": "[MRG] ENH enable setting pipeline components as parameters (#1769)\n\nPipeline and FeatureUnion steps may now be set with set_params, and transformers may be replaced with None to effectively remove them.\r\n\r\nAlso test and improve ducktyping of Pipeline methods\r\n",
      "commitDate": "2016/8/29 下午9:32",
      "commitName": "5b20d484add50aec64a1bda5c52ed2ceb7557f36",
      "commitAuthor": "Joel Nothman",
      "diff": "@@ -0,0 +1,19 @@\n+    def _fit(self, X, y\u003dNone, **fit_params):\n+        self._validate_steps()\n+        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n+                                if step is not None)\n+        for pname, pval in six.iteritems(fit_params):\n+            step, param \u003d pname.split(\u0027__\u0027, 1)\n+            fit_params_steps[step][param] \u003d pval\n+        Xt \u003d X\n+        for name, transform in self.steps[:-1]:\n+            if transform is None:\n+                pass\n+            elif hasattr(transform, \"fit_transform\"):\n+                Xt \u003d transform.fit_transform(Xt, y, **fit_params_steps[name])\n+            else:\n+                Xt \u003d transform.fit(Xt, y, **fit_params_steps[name]) \\\n+                              .transform(Xt)\n+        if self._final_estimator is None:\n+            return Xt, {}\n+        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "actualSource": "    def _fit(self, X, y\u003dNone, **fit_params):\n        self._validate_steps()\n        fit_params_steps \u003d dict((name, {}) for name, step in self.steps\n                                if step is not None)\n        for pname, pval in six.iteritems(fit_params):\n            step, param \u003d pname.split(\u0027__\u0027, 1)\n            fit_params_steps[step][param] \u003d pval\n        Xt \u003d X\n        for name, transform in self.steps[:-1]:\n            if transform is None:\n                pass\n            elif hasattr(transform, \"fit_transform\"):\n                Xt \u003d transform.fit_transform(Xt, y, **fit_params_steps[name])\n            else:\n                Xt \u003d transform.fit(Xt, y, **fit_params_steps[name]) \\\n                              .transform(Xt)\n        if self._final_estimator is None:\n            return Xt, {}\n        return Xt, fit_params_steps[self.steps[-1][0]]\n",
      "path": "sklearn/pipeline.py",
      "functionStartLine": 222,
      "functionName": "_fit",
      "functionAnnotation": ""
    }
  }
}